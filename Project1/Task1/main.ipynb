{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from src.tokenization import *\n",
    "from src.vectorization import *\n",
    "\n",
    "F_reduced_dataset = False  # If true load only 1% of corpus and small portion of queries and train_set\n",
    "F_do_tokenization = False  # If true tokenize corpus + queries, else load already tokenized documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "corpus, queries, train_set, test_set = load_task1_data(F_reduced_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "token_dir = \"Data/tokens/\"\n",
    "\n",
    "# Tokenize corpus and queries and save it, OR load it\n",
    "if F_do_tokenization:\n",
    "    tokenize_corpus_queries(corpus, queries, stemmer)\n",
    "    save_tokenized_corpus_queries(token_dir, corpus, queries)\n",
    "else:\n",
    "    load_tokenized_corpus_queries(token_dir, corpus, queries)\n",
    "\n",
    "vectorized_corpus, vectorized_queries = vectorize_corpus_queries(corpus, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find top 10 relevant document for each test query\n",
    "vectorized_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Load the data with corrected file paths\n",
    "corpus = pd.read_json(\"Data/raw/corpus.jsonl\", lines=True)\n",
    "queries = pd.read_json(\"Data/raw/queries.jsonl\", lines=True)\n",
    "test_1 = pd.read_csv(\"Data/raw/task1_test.tsv\", delimiter='\\t')\n",
    "\n",
    "unique_query_ids = set(test_1['query-id'])\n",
    "filtered_queries = queries[queries['_id'].isin(unique_query_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Data preprocessing and feature engineering can be done here.\n",
    "# Vectorize the corpus and queries as sparse matrices\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "corpus_tfidf = csr_matrix(tfidf_vectorizer.fit_transform(corpus['text']))\n",
    "query_tfidf = csr_matrix(tfidf_vectorizer.transform(filtered_queries['text']))\n",
    "\n",
    "clf = TruncatedSVD(300)\n",
    "corpus_pca = clf.fit_transform(corpus_tfidf)\n",
    "query_pca = clf.transform(query_tfidf)\n",
    "print(f\"Variance explained : {clf.explained_variance_ratio_.cumsum()[-1]*100:2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "batch_size = 50000\n",
    "num_queries = query_pca.shape[0]\n",
    "num_corpus_docs = corpus_pca.shape[0]\n",
    "corpus_ids = corpus['_id'].to_list()\n",
    "\n",
    "# Initialize arrays to store the top 10 document indices and their similarity scores\n",
    "top_10_indices = np.zeros((num_queries, 0), dtype=int)\n",
    "top_10_similarities = np.zeros((num_queries, 0))\n",
    "\n",
    "for i in range(0, num_corpus_docs, batch_size):\n",
    "    start = i\n",
    "    end = min(i + batch_size, num_corpus_docs)\n",
    "    batch_corpus_pca = corpus_pca[start:end]\n",
    "    batch_similarity = cosine_similarity(query_pca, batch_corpus_pca)\n",
    "\n",
    "    # Find the top 10 indices and their similarity scores for each query in this batch\n",
    "    top_10_batch_indices = np.argpartition(batch_similarity, -top_k, axis=1)[:, -top_k:]\n",
    "    top_10_batch_corpus_indices = np.array(corpus_ids[start:end])[top_10_batch_indices]\n",
    "    top_10_batch_similarities = np.partition(batch_similarity, -top_k, axis=1)[:, -top_k:]\n",
    "\n",
    "    # Update the top_10_indices and top_10_similarities arrays\n",
    "    top_10_indices = np.hstack((top_10_indices, top_10_batch_corpus_indices))\n",
    "    top_10_similarities = np.hstack((top_10_similarities, top_10_batch_similarities))\n",
    "    if i%100000==0:\n",
    "        print(f\"{i/num_corpus_docs*100:.2f}%\")\n",
    "\n",
    "# Now, top_10_indices contains the top 10 document indices for each query, and\n",
    "# top_10_similarities contains their similarity scores.\n",
    "\n",
    "indices = np.argpartition(top_10_similarities, -top_k, axis=1)[:, -top_k:]\n",
    "overall_top_10_indices = []\n",
    "for i in range(len(top_10_indices)):\n",
    "    overall_top_10_indices.append(top_10_indices[i][indices[i]])\n",
    "print(\"100%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_top_10_indices = []\n",
    "for i in range(len(top_10_indices)):\n",
    "    overall_top_10_indices.append(top_10_indices[i][indices[i]])\n",
    "overall_top_10_indices = np.array(overall_top_10_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_top_10_indices[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
