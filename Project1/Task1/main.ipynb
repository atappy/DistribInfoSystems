{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import pickle\n",
    "from src.utils import *\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from scipy.sparse.linalg import norm as snorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "corpus, queries, train_set, test_set = load_task1_data(F_reduced_dataset = False)\n",
    "\n",
    "# Only queries in test_set\n",
    "unique_query_ids = set(test_set.to_list())\n",
    "queries = queries[queries.index.isin(unique_query_ids)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "token_dir = \"Data/tokens/\"\n",
    "\n",
    "# Tokenize corpus and queries and save it, OR load it\n",
    "if False:\n",
    "    tokenize_corpus_queries(corpus, queries, stemmer)\n",
    "    save_tokenized_corpus_queries(token_dir, corpus, queries)\n",
    "else:\n",
    "    corpus, queries = load_tokenized_corpus_queries(token_dir, corpus, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count presence of each term in each document\n",
    "vocab = sorted(list(set([token for tokens in corpus['tokens'].to_list() for token in tokens])))\n",
    "if True:\n",
    "    count_corpus, count_queries = count_terms(corpus, queries, vocab)\n",
    "\n",
    "    with open(\"Data/tfidf/count_corpus.npz\",\"wb\") as f:\n",
    "        pickle.dump(count_corpus, f)\n",
    "    with open(\"Data/tfidf/count_queries.npz\",\"wb\") as f:\n",
    "        pickle.dump(count_queries, f)\n",
    "\n",
    "else:\n",
    "    with open(\"Data/tfidf/count_corpus.npz\",\"rb\") as f:\n",
    "        count_corpus = pickle.load(f)\n",
    "    with open(\"Data/tfidf/count_queries.npz\",\"rb\") as f:\n",
    "        count_queries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions and indices\n",
    "T = len(vocab)\n",
    "D = len(corpus)\n",
    "Q = len(queries)\n",
    "doc_indices, term_indices = count_corpus.nonzero()\n",
    "indices = np.array((doc_indices, term_indices)).swapaxes(0,1)\n",
    "\n",
    "# N doc with term t\n",
    "n_doc_with_term = np.zeros((T))\n",
    "for i in term_indices:\n",
    "    n_doc_with_term[i] +=1\n",
    "\n",
    "# N of different term for each doc, + mean\n",
    "n_different_term = np.zeros((D))\n",
    "for i in doc_indices:\n",
    "    n_different_term[i] +=1\n",
    "n_unique_avg = np.mean(n_different_term) # 1\n",
    "\n",
    "# Highest count\n",
    "highest_count = np.zeros((D)) # D\n",
    "for (d,t) in indices:\n",
    "    a = highest_count[d]\n",
    "    b = count_corpus[d,t]\n",
    "    if a < b:\n",
    "        highest_count[d] = b\n",
    "\n",
    "# IDF\n",
    "log_D = np.full(shape=(T), fill_value=np.log(D))\n",
    "idf = log_D - n_doc_with_term # T\n",
    "\n",
    "# TF\n",
    "s = 0.2\n",
    "tf_corpus = lil_matrix((D,T), dtype=np.float32)\n",
    "for (d,t) in indices:\n",
    "    up = count_corpus[d,t]/highest_count[d]\n",
    "    below = ((1-s)*n_unique_avg + s*n_different_term[d])\n",
    "    tf_corpus[d,t]=up/below\n",
    "\n",
    "# TFIDF\n",
    "tfidf_corpus = lil_matrix((D,T), dtype=np.float32)\n",
    "for (d,t) in indices:\n",
    "    tfidf_corpus[d,t] = tf_corpus[d,t]*idf[t]\n",
    "\n",
    "with open(\"Data/tfidf/tfidf_corpus.npz\",\"wb\") as f:\n",
    "        pickle.dump(tfidf_corpus, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indices\n",
    "query_indices, term_indices = count_queries.nonzero()\n",
    "indices = np.array((query_indices, term_indices)).swapaxes(0,1)\n",
    "\n",
    "# Highest_count\n",
    "highest_count = np.zeros((Q)) \n",
    "for (q,t) in indices:\n",
    "    a = highest_count[q]\n",
    "    b = count_queries[q,t]\n",
    "    if a < b:\n",
    "        highest_count[q] = b\n",
    "\n",
    "# N of different term for each doc, + mean\n",
    "n_different_term = np.zeros((Q))\n",
    "for i in query_indices:\n",
    "    n_different_term[i] +=1\n",
    "\n",
    "# TF\n",
    "s = 0.2\n",
    "tf_queries = lil_matrix((Q,T), dtype=np.float32)\n",
    "for (q,t) in indices:\n",
    "    up = count_queries[q,t]/highest_count[q]\n",
    "    below = ((1-s)*n_unique_avg + s*n_different_term[q])\n",
    "    tf_queries[q,t]=up/below\n",
    "\n",
    "# TFIDF\n",
    "tfidf_queries = lil_matrix((Q,T), dtype=np.float32)\n",
    "for (q,t) in indices:\n",
    "    tfidf_queries[q,t] = tf_queries[q,t]*idf[t]\n",
    "\n",
    "with open(\"Data/tfidf/tfidf_queries.npz\",\"wb\") as f:\n",
    "    pickle.dump(tfidf_queries, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Data/tfidf/tfidf_corpus.npz\",\"rb\") as f:\n",
    "    tfidf_corpus = pickle.load(f)\n",
    "with open(\"Data/tfidf/tfidf_queries.npz\",\"rb\") as f:\n",
    "    tfidf_queries = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# norms\n",
    "D = tfidf_corpus.shape[0]\n",
    "Q = tfidf_queries.shape[0]\n",
    "\n",
    "tfidf_corpus = csr_matrix(tfidf_corpus)\n",
    "tfidf_queries = csr_matrix(tfidf_queries)\n",
    "\n",
    "corpus_norms = np.zeros((D))\n",
    "for i in range(D):\n",
    "    corpus_norms[i] = snorm(tfidf_corpus[i])\n",
    "\n",
    "queries_norms = np.zeros((Q))\n",
    "for i in range(Q):\n",
    "    queries_norms[i] = snorm(tfidf_queries[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_top = 10\n",
    "similarities = lil_matrix((D,Q), dtype=np.float32)\n",
    "doc_indices, doc_term_indices = tfidf_corpus.nonzero()\n",
    "corpus_indices = np.array((doc_indices, doc_term_indices)).swapaxes(0,1)\n",
    "query_indices, query_term_indices = tfidf_queries.nonzero()\n",
    "queries_indices = np.array((query_indices, query_term_indices)).swapaxes(0,1)\n",
    "\n",
    "for q in range(Q):\n",
    "    print(q)\n",
    "    non_zero_q = queries_indices[query_indices==q]\n",
    "    if len(non_zero_q) == 0:\n",
    "        continue\n",
    "    for d in range(D):\n",
    "        non_zero_d = corpus_indices[doc_indices==d]\n",
    "        if len(non_zero_d) == 0:\n",
    "            continue\n",
    "        non_zero_coos_index = np.array(np.intersect1d(non_zero_q[:,1], non_zero_d[:,1], return_indices=True))\n",
    "        if np.prod(non_zero_coos_index.shape) == 0:\n",
    "            continue\n",
    "        for t in non_zero_coos_index:\n",
    "            t = t[0]\n",
    "            similarities[d,q] += tfidf_corpus[d,t] * tfidf_queries[q,t]\n",
    "        if similarities[d,q]!=0:\n",
    "            similarities[d,q] /= queries_norms[q] * corpus_norms[d]\n",
    "    \n",
    "\"\"\"\n",
    "    similarities_q.append(tfidf_corpus[d] @ tfidf_queries[q].T / queries_norms[q] / corpus_norms[d])\n",
    "    print(d)\n",
    "    similarities_indices[q] = np.argsort(similarities_q)[::-1][:k_top]\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TEST PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD, IncrementalPCA\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data with corrected file paths\n",
    "corpus = pd.read_json(\"Data/raw/corpus.jsonl\", lines=True)\n",
    "queries = pd.read_json(\"Data/raw/queries.jsonl\", lines=True)\n",
    "test_1 = pd.read_csv(\"Data/raw/task1_test.tsv\", delimiter='\\t')\n",
    "\n",
    "unique_query_ids = set(test_1['query-id'])\n",
    "filtered_queries = queries[queries['_id'].isin(unique_query_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing and feature engineering can be done here.\n",
    "# Vectorize the corpus and queries as sparse matrices\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "corpus_tfidf = csr_matrix(tfidf_vectorizer.fit_transform(corpus['text']))\n",
    "query_tfidf = csr_matrix(tfidf_vectorizer.transform(filtered_queries['text']))\n",
    "\n",
    "n_dim=500\n",
    "clf = IncrementalPCA(n_components = n_dim, batch_size=500)\n",
    "corpus_pca = clf.fit_transform(corpus_tfidf)\n",
    "query_pca = clf.transform(query_tfidf)\n",
    "print(f\"Variance explained : {clf.explained_variance_ratio_.cumsum()[-1]*100:2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data preprocessing and feature engineering can be done here.\n",
    "# Vectorize the corpus and queries as sparse matrices\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "corpus_tfidf = csr_matrix(tfidf_vectorizer.fit_transform(corpus['text']))\n",
    "query_tfidf = csr_matrix(tfidf_vectorizer.transform(filtered_queries['text']))\n",
    "\n",
    "n_dim=600\n",
    "clf = TruncatedSVD(n_dim, algorithm='arpack')\n",
    "corpus_pca = clf.fit_transform(corpus_tfidf)\n",
    "query_pca = clf.transform(query_tfidf)\n",
    "print(f\"Variance explained : {clf.explained_variance_ratio_.cumsum()[-1]*100:2f}%\") # env. 24% for n_dim = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('corpus_pca_600.npy', 'wb') as f:\n",
    "    np.save(f, corpus_pca )\n",
    "with open('query_pca_600.npy', 'wb') as f:\n",
    "    np.save(f,query_pca )\n",
    "\n",
    "with open('corpus_pca_600.npy', 'rb') as f:\n",
    "    corpus_pca = np.load(f)\n",
    "with open('query_pca_600.npy', 'rb') as f:\n",
    "    query_pca = np.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 10\n",
    "batch_size = 50000\n",
    "num_queries = query_pca.shape[0]\n",
    "num_corpus_docs = corpus_pca.shape[0]\n",
    "corpus_ids = corpus['_id'].to_list()\n",
    "\n",
    "# Initialize arrays to store the top 10 document indices and their similarity scores\n",
    "top_10_indices = np.zeros((num_queries, 0), dtype=int)\n",
    "top_10_similarities = np.zeros((num_queries, 0))\n",
    "\n",
    "for i in range(0, num_corpus_docs, batch_size):\n",
    "    start = i\n",
    "    end = min(i + batch_size, num_corpus_docs)\n",
    "    batch_corpus_pca = corpus_pca[start:end]\n",
    "    batch_similarity = cosine_similarity(query_pca, batch_corpus_pca)\n",
    "\n",
    "    # Find the top 10 indices and their similarity scores for each query in this batch\n",
    "    top_10_batch_indices = np.argpartition(batch_similarity, -top_k, axis=1)[:, -top_k:]\n",
    "    top_10_batch_corpus_indices = np.array(corpus_ids[start:end])[top_10_batch_indices]\n",
    "    top_10_batch_similarities = np.partition(batch_similarity, -top_k, axis=1)[:, -top_k:]\n",
    "\n",
    "    # Update the top_10_indices and top_10_similarities arrays\n",
    "    top_10_indices = np.hstack((top_10_indices, top_10_batch_corpus_indices))\n",
    "    top_10_similarities = np.hstack((top_10_similarities, top_10_batch_similarities))\n",
    "    if i%100000==0:\n",
    "        print(f\"{i/num_corpus_docs*100:.2f}%\")\n",
    "\n",
    "# Now, top_10_indices contains the top 10 document indices for each query, and\n",
    "# top_10_similarities contains their similarity scores.\n",
    "\n",
    "indices = np.argpartition(top_10_similarities, -top_k, axis=1)[:, -top_k:]\n",
    "overall_top_10_indices = []\n",
    "for i in range(len(top_10_indices)):\n",
    "    overall_top_10_indices.append(top_10_indices[i][indices[i]])\n",
    "print(\"100%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_top_10_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ADA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
