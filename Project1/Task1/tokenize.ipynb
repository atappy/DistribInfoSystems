{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project 1 - Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/aducret/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "from operator import itemgetter\n",
    "from collections import Counter\n",
    "nltk.download('stopwords')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "FLAG_TOKENIZE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data + tokenization and stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize, stem a document\n",
    "def tokenize(text):\n",
    "    text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return \",\".join([stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')])\n",
    "\n",
    "# Import corpus and queries as a dataframe\n",
    "corpus = pd.read_json('Data/corpus.jsonl', lines=True)\n",
    "queries = pd.read_json('Data/queries.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize corpus and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FLAG_TOKENIZE == True:\n",
    "    # Tokenize, stem a document\n",
    "    def tokenize(text):\n",
    "        text = \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return \",\".join([stemmer.stem(word.lower()) for word in tokens if word not in stopwords.words('english')])\n",
    "\n",
    "    corpus['tokens']  = corpus['text'].apply(lambda x: tokenize(x))\n",
    "    queries['tokens'] = queries['text'].apply(lambda x: tokenize(x))\n",
    "\n",
    "    corpus[['_id','tokens']].to_csv('Data/corpus_tokens.txt', sep=',', index=False)\n",
    "    queries[['_id','tokens']].to_csv('Data/queries_tokens.txt', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read tokenize file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       _id                                             tokens\n",
      "0  1867825  after,invent,cotton,gin,cotton,becam,americaâ...\n",
      "1   419610  timer,separ,night,day,outlet,nice,time,set,rot...\n",
      "2  4614226  the,rosebuy,public,still,encount,wide,varieti,...\n",
      "       _id                                             tokens\n",
      "0  1185869            immedi,impact,success,manhattan,project\n",
      "1  1185868  justic,design,repair,harm,victim,commun,offend...\n",
      "2   597651                                   color,amber,urin\n",
      "       _id                                             tokens\n",
      "0  1867825  [after, invent, cotton, gin, cotton, becam, am...\n",
      "1   419610  [timer, separ, night, day, outlet, nice, time,...\n",
      "2  4614226  [the, rosebuy, public, still, encount, wide, v...\n"
     ]
    }
   ],
   "source": [
    "corpus_tokens = pd.read_csv('Data/corpus_tokens.txt')\n",
    "queries_tokens = pd.read_csv('Data/queries_tokens.txt')\n",
    "\n",
    "print(corpus_tokens.head(3))\n",
    "print(queries_tokens.head(3))\n",
    "\n",
    "# string to list of string\n",
    "corpus_tokens['tokens'] = corpus_tokens['tokens'].apply(lambda x : x.split(\",\"))\n",
    "\n",
    "print(corpus_tokens.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# string to list\n",
    "\n",
    "vocabulary = list(set([ x for y in corpus_tokens['tokens'] for x in y]))\n",
    "vocabulary.sort()\n",
    "\n",
    "with open(r'Data/vocabulary.txt', 'w') as fp:\n",
    "    for item in vocabulary:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create vocabulary with cheat method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_documents = [x for x in corpus['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/aducret/Documents/EPFL/DIS_Course/DistribInfoSystems/Project1/Task1/Tokenize.ipynb Cell 14\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aducret/Documents/EPFL/DIS_Course/DistribInfoSystems/Project1/Task1/Tokenize.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m tf \u001b[39m=\u001b[39m TfidfVectorizer(analyzer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mword\u001b[39m\u001b[39m'\u001b[39m, ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m), min_df \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, stop_words \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aducret/Documents/EPFL/DIS_Course/DistribInfoSystems/Project1/Task1/Tokenize.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m original_documents \u001b[39m=\u001b[39m corpus[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/aducret/Documents/EPFL/DIS_Course/DistribInfoSystems/Project1/Task1/Tokenize.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m features \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mfit_transform(original_documents)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aducret/Documents/EPFL/DIS_Course/DistribInfoSystems/Project1/Task1/Tokenize.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m vocabulary_cheat \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(tf\u001b[39m.\u001b[39mvocabulary_\u001b[39m.\u001b[39mkeys())\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/aducret/Documents/EPFL/DIS_Course/DistribInfoSystems/Project1/Task1/Tokenize.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m vocabulary_cheat\u001b[39m.\u001b[39msort()\n",
      "File \u001b[0;32m~/anaconda3/envs/ADA/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:2139\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[1;32m   2133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[1;32m   2134\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[1;32m   2135\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[1;32m   2136\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[1;32m   2137\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[1;32m   2138\u001b[0m )\n\u001b[0;32m-> 2139\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[1;32m   2140\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[1;32m   2141\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[1;32m   2142\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/ADA/lib/python3.11/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/ADA/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1406\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1402\u001b[0m     X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_limit_features(\n\u001b[1;32m   1403\u001b[0m         X, vocabulary, max_doc_count, min_doc_count, max_features\n\u001b[1;32m   1404\u001b[0m     )\n\u001b[1;32m   1405\u001b[0m     \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1406\u001b[0m         X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sort_features(X, vocabulary)\n\u001b[1;32m   1407\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocabulary_ \u001b[39m=\u001b[39m vocabulary\n\u001b[1;32m   1409\u001b[0m \u001b[39mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/anaconda3/envs/ADA/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1213\u001b[0m, in \u001b[0;36mCountVectorizer._sort_features\u001b[0;34m(self, X, vocabulary)\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[39mfor\u001b[39;00m new_val, (term, old_val) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(sorted_features):\n\u001b[1;32m   1212\u001b[0m     vocabulary[term] \u001b[39m=\u001b[39m new_val\n\u001b[0;32m-> 1213\u001b[0m     map_index[old_val] \u001b[39m=\u001b[39m new_val\n\u001b[1;32m   1215\u001b[0m X\u001b[39m.\u001b[39mindices \u001b[39m=\u001b[39m map_index\u001b[39m.\u001b[39mtake(X\u001b[39m.\u001b[39mindices, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mclip\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1216\u001b[0m \u001b[39mreturn\u001b[39;00m X\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "original_documents = corpus['text']\n",
    "features = tf.fit_transform(original_documents)\n",
    "\n",
    "vocabulary_cheat = list(tf.vocabulary_.keys())\n",
    "vocabulary_cheat.sort()\n",
    "\n",
    "with open(r'vocabulary_cheat.txt', 'w') as fp:\n",
    "    for item in vocabulary:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
