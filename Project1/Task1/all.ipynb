{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. Imports and flags\n",
    "2. Tokenization\n",
    "3. Vectorization\n",
    "4. Search with query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Imports and flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/atappy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/atappy/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from operator import itemgetter\n",
    "from collections import Counter, defaultdict\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Flags <span style=\"color:red\"> !!! CHECK BEFORE RUNING EACH CELLS !!! </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_reduced_dataset = True  # If true load only 1% of corpus and small portion of queries and train_set\n",
    "F_do_tokenization = True # If true tokenize corpus + queries, else load already tokenized documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_json('Data/corpus.jsonl', lines=True).set_index(['_id'])\n",
    "queries = pd.read_json('Data/queries.jsonl', lines=True)[['_id', 'text']].set_index(['_id'])\n",
    "train_set = pd.read_table(\"Data/task1_train.tsv\")[[\"query-id\",\"corpus-id\"]].set_index(['query-id'])\n",
    "\n",
    "if F_reduced_dataset:\n",
    "    corpus = corpus.head(15000)\n",
    "    queries = queries.head(15000)\n",
    "    train_set = train_set.head(5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus \n",
      "                                                       text\n",
      "_id                                                       \n",
      "1867825  After the invention of the cotton gin, cotton ...\n",
      "419610   Timer has separate night and day outlets, whic...\n",
      "4614226  The rose-buying public still encounters a wide... \n",
      "\n",
      "queries \n",
      "                                                       text\n",
      "_id                                                       \n",
      "1185869  )what was the immediate impact of the success ...\n",
      "1185868  _________ justice is designed to repair the ha...\n",
      "597651                           what color is amber urine \n",
      "\n",
      "train_set \n",
      "           corpus-id\n",
      "query-id           \n",
      "1185869           0\n",
      "1185868          16\n",
      "597651           49\n"
     ]
    }
   ],
   "source": [
    "# Display data\n",
    "print(\"corpus\", \"\\n\", corpus.head(3), \"\\n\")\n",
    "print(\"queries\", \"\\n\", queries.head(3), \"\\n\")\n",
    "print(\"train_set\", \"\\n\", train_set.head(3) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Tokenization tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Remove any punctuation charachter from a text (str)\n",
    "    \"\"\"\n",
    "    return \"\".join([ch for ch in text if ch not in string.punctuation])\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Transform a text (str) in a list of stemmed words (list[str])\n",
    "    \"\"\"\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [remove_punctuation(token) for token in tokens]\n",
    "    tokens = [word.lower() for word in tokens if word.lower() not in stopwords.words('english') and len(word)>1]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Tokenize corpus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize corpus and queries\n",
    "if F_do_tokenization:\n",
    "    corpus['tokens'] = corpus['text'].apply(lambda x: tokenize(x))\n",
    "    queries['tokens'] = queries['text'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files saved\n"
     ]
    }
   ],
   "source": [
    "token_dir = \"Data/tokens/\"\n",
    "\n",
    "# Save tokenized corpus by splitting into gittable/small files\n",
    "if F_do_tokenization:\n",
    "    n_sample_per_file = 70000\n",
    "    n_sample = len(corpus)\n",
    "    n_files =  n_sample//n_sample_per_file + 1\n",
    "\n",
    "    for i in range(0, n_files):\n",
    "        first = i*n_sample_per_file\n",
    "        last = min(first + n_sample_per_file, n_sample)\n",
    "        corpus_tokens_part = corpus[\"tokens\"].iloc[first:last]\n",
    "        \n",
    "        filename = f\"{token_dir}corpus_tokens_{i:02d}.pkl\"\n",
    "        corpus_tokens_part.to_pickle(filename)\n",
    "    \n",
    "    queries[\"tokens\"].to_pickle(f\"{token_dir}queries_tokens.pkl\")\n",
    "    print(\"Files saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenized corpus as a dataframe\n",
    "if not F_do_tokenization:\n",
    "    files_paths=os.listdir(token_dir)\n",
    "    corpus_tokens_paths= [f\"{token_dir+path}\" for path in files_paths if 'corpus_tokens' in path and \".pkl\" in path]\n",
    "    corpus_tokens_paths.sort()\n",
    "    dfs = [pd.read_pickle(path) for path in corpus_tokens_paths]\n",
    "    corpus[\"tokens\"] = pd.concat(dfs)\n",
    "    queries[\"tokens\"] = pd.read_pickle(f\"{token_dir}queries_tokens.pkl\")\n",
    "    print(\"Files loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Explore vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.0 Check vocab by document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_index = 4\n",
    "tokens = corpus.iloc[0:vocab_index]['tokens']\n",
    "vocab_custom = sorted(list(set([ x for y in tokens for x in y])))\n",
    "texts = corpus.iloc[0:vocab_index]['text']\n",
    "\n",
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = stopwords.words('english') )\n",
    "tf_matrix = tf.fit_transform(texts)\n",
    "vocab_cheat = sorted(list(tf.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclusive to custom vocab\n",
      "12\n",
      "1500\n",
      "35000\n",
      "americaâs\n",
      "highend\n",
      "rosebuying\n"
     ]
    }
   ],
   "source": [
    "print(\"exclusive to custom vocab\")\n",
    "for v in vocab_custom :\n",
    "    if v not in vocab_cheat:\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exclusive to cheat vocab\n",
      "000\n",
      "35\n",
      "500\n",
      "americaâ\n",
      "buying\n",
      "end\n",
      "high\n",
      "rose\n"
     ]
    }
   ],
   "source": [
    "print(\"exclusive to cheat vocab\")\n",
    "for v in vocab_cheat :\n",
    "    if v not in vocab_custom :\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.1 Custom vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set([ x for y in corpus['tokens'] for x in y]))\n",
    "vocabulary.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2 Cheat vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = TfidfVectorizer(analyzer='word', ngram_range=(1,1), min_df = 1, stop_words = 'english')\n",
    "tf.fit_transform(corpus['text'])\n",
    "vocabulary_cheat = sorted(list(tf.vocabulary_.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Save vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "with open(r'Data/vocabulary.txt', 'w') as fp:\n",
    "    for item in vocabulary:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(r'Data/vocabulary_cheat.txt', 'w') as fp:\n",
    "    for item in vocabulary_cheat:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Define vectorization functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1 Inverse Document Frequency - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(documents):\n",
    "    \"\"\"\n",
    "    Compute Inverse Document Frequency (IDF) for each term in all documents \n",
    "    \n",
    "    documents : numpy array of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    total_documents = len(documents)\n",
    "    word_document_count = defaultdict(int)\n",
    "\n",
    "    average_number_words = 0 #global variable to compute average number of distinct words per document\n",
    "\n",
    "    for document in documents:\n",
    "        unique_words = set(document)\n",
    "        average_number_words += len(unique_words)\n",
    "        for word in unique_words:\n",
    "            word_document_count[word] += 1\n",
    "    average_number_words = average_number_words / total_documents\n",
    "\n",
    "    idf = {}\n",
    "    for word, count in word_document_count.items():\n",
    "        idf[word] = math.log(total_documents / (count))\n",
    "\n",
    "    return average_number_words, defaultdict(float,idf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 Term frequency - TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(document, average_number_words, s = 0.2):\n",
    "    \"\"\"\n",
    "    Compute Term Frequency (TF) for each term in a document and normalize it using the pivoted unique query normalization\n",
    "\n",
    "    document : \n",
    "    average_number_words :\n",
    "    s : normalization parameter\n",
    "\n",
    "    \"\"\"\n",
    "    word_counts = Counter(document)\n",
    "    unique_words_count = len(set(document)) # TODO : Use already computed unique_words_count in IDF ?\n",
    "    tf = {word: (count / max(word_counts.values())) / ((1.0-s)*average_number_words + s*unique_words_count) for word, count in word_counts.items()}\n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3 Vectorize a document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tokens, idf, average_number_words):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF weights for each term in all documents -> vectorize each document\n",
    "    \"\"\"\n",
    "    vector = {}\n",
    "    tf = compute_tf(tokens, average_number_words)\n",
    "    vector.update({word: tf[word] * idf[word] for word in tf.keys()})\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Vectorize corpus and queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_tokens_array = corpus['tokens'].to_numpy()\n",
    "queries_tokens_array = queries['tokens'].to_numpy()\n",
    "average_number_words, idf = compute_idf(corpus_tokens_array)\n",
    "vectorized_corpus = [vectorize(document_tokens, idf, average_number_words) for document_tokens in corpus_tokens_array]\n",
    "vectorized_queries = [vectorize(query_tokens, idf, average_number_words) for query_tokens in queries_tokens_array]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Search with query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Define searching function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1 Cosine similarity function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute cosine similarity\n",
    "def cosine_similarity(v1,v2):\n",
    "    sumxx, sumxy, sumyy = 0, 0, 0\n",
    "# y to default dict ?\n",
    "    v2 = defaultdict(float, v2)\n",
    "    for i in v1:\n",
    "        x = v1[i]; y = v2[i]\n",
    "        sumxx += x*x\n",
    "        sumyy += y*y\n",
    "        sumxy += x*y\n",
    "    if sumxy == 0:\n",
    "            result = 0\n",
    "    else:\n",
    "            result = sumxy/math.sqrt(sumxx*sumyy)\n",
    "    return  result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Find k most relevant document for query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_search(query_vector, vectorized_corpus, corpus_ids, k=10):\n",
    "    similarities = np.array([cosine_similarity(query_vector, doc_vec) for doc_vec in vectorized_corpus])\n",
    "    corpus_ids = corpus_ids[similarities.argsort()[::-1]]\n",
    "\n",
    "    return (corpus_ids[:k].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Example : Search for 1st query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m query_vector \u001b[38;5;241m=\u001b[39m vectorized_queries[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(k_search(query_vector, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43midf_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, corpus\u001b[38;5;241m.\u001b[39mindex))\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "query_vector = vectorized_queries[0]\n",
    "print(k_search(query_vector, tf.idf_(), corpus.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Find most relevant documents for all queries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_id, query_vector in zip(queries.index.to_list(), vectorized_queries):\n",
    "    docs_id = k_search(query_vector, vectorized_corpus, corpus.index, 1)\n",
    "    true_doc_id = train_set.loc[q_id][\"corpus-id\"]\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
